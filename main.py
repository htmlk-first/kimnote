import os
import tempfile
from typing import List, TypedDict

import streamlit as st
from dotenv import load_dotenv

# LangChain & LangGraph Imports
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS

from langchain_community.retrievers import BM25Retriever
from langchain_classic.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain_classic.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.graph import StateGraph, END, START

# 1. í™˜ê²½ ì„¤ì • ë¡œë“œ
load_dotenv()

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="UAV ì—°êµ¬ ë³´ì¡° RAG", page_icon="ğŸš")
st.title("UAV ì—°êµ¬ ë³´ì¡° Agentic RAG")

# ==============================================================================
# [Part 1] ìºì‹±ëœ RAG ì‹œìŠ¤í…œ ë¹Œë”
# ==============================================================================


@st.cache_resource
def initialize_models():
    """ëª¨ë¸ ë¡œë“œëŠ” ì‹œê°„ì´ ê±¸ë¦¬ë¯€ë¡œ ìºì‹± ì²˜ë¦¬"""
    embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")
    llm_model = ChatOpenAI(model="gpt-4o", temperature=0)
    # Reranker ëª¨ë¸ (ìµœì´ˆ ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œ)
    reranker_model = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-v2-m3")
    return embedding_model, llm_model, reranker_model


EMBEDDING_MODEL, LLM_MODEL, RERANKER_MODEL = initialize_models()


def build_retriever(file_path: str):
    """PDF íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ Retriever ìƒì„±"""
    with st.status("ğŸ“„ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘...", expanded=True) as status:
        st.write("1. PDF ë¬¸ì„œ ë¡œë“œ ì¤‘...")
        loader = PyMuPDFLoader(file_path)
        docs = loader.load()

        st.write("2. í…ìŠ¤íŠ¸ ë¶„í•  ë° ì²­í‚¹...")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        splits = text_splitter.split_documents(docs)
        texts = [d.page_content for d in splits]

        if not texts:
            status.update(
                label="âš ï¸ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", state="error", expanded=True
            )
            return None

        st.write("3. Vector Index (Dense) ìƒì„± ì¤‘...")
        vectorstore = FAISS.from_texts(texts, EMBEDDING_MODEL)
        vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

        st.write("4. BM25 Index (Sparse) ìƒì„± ì¤‘...")
        bm25_retriever = BM25Retriever.from_texts(texts)
        bm25_retriever.k = 5

        st.write("5. Ensemble ë° Reranker ì„¤ì •...")
        ensemble_retriever = EnsembleRetriever(
            retrievers=[bm25_retriever, vector_retriever],
            weights=[0.4, 0.6],
        )

        compressor = CrossEncoderReranker(model=RERANKER_MODEL, top_n=3)
        final_retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=ensemble_retriever,
        )

        status.update(
            label="âœ… RAG ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!", state="complete", expanded=False
        )

    return final_retriever


# ==============================================================================
# [Part 2] LangGraph Workflow ì •ì˜
# ==============================================================================


class GraphState(TypedDict, total=False):
    question: str
    documents: List[str]
    generation: str
    sub_queries: List[str]


def create_rag_graph(retriever):
    """Retrieverê°€ ì£¼ì…ëœ LangGraph ì•± ìƒì„±"""

    # --- ë…¸ë“œ í•¨ìˆ˜ ---
    def query_decomposition_node(state: GraphState) -> GraphState:
        question = state["question"]
        prompt = ChatPromptTemplate.from_template(
            "ì§ˆë¬¸ì„ ê²€ìƒ‰í•˜ê¸° ì¢‹ì€ 2ê°œì˜ í•œêµ­ì–´ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„ë¦¬í•´ì¤˜. "
            "ê²°ê³¼ëŠ” ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•´.\nì§ˆë¬¸: {question}"
        )
        chain = prompt | LLM_MODEL | StrOutputParser()
        response = chain.invoke({"question": question})
        sub_queries = [q.strip() for q in response.split("\n") if q.strip()]
        return {"sub_queries": sub_queries}

    def retrieval_node(state: GraphState) -> GraphState:
        sub_queries = state.get("sub_queries", [])
        all_docs: List[str] = []

        for q in sub_queries:
            docs = retriever.invoke(q)
            for d in docs:
                all_docs.append(d.page_content)

        # ì¤‘ë³µ ì œê±°
        unique_docs = list(set(all_docs))
        return {"documents": unique_docs}

    def grade_documents_node(state: GraphState) -> GraphState:
        # ê°„ì†Œí™”ëœ í‰ê°€ ë¡œì§ (ê·¸ëŒ€ë¡œ í†µê³¼)
        return state

    def generate_node(state: GraphState) -> GraphState:
        question = state["question"]
        documents = state.get("documents", [])
        context = "\n\n".join(documents)

        prompt = ChatPromptTemplate.from_template(
            "ì•„ë˜ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ì—°êµ¬ì›ì—ê²Œ ë³´ê³ í•˜ë“¯ ìƒì„¸íˆ ë‹µë³€í•´ì¤˜.\n\n"
            "[ë¬¸ì„œ]\n{context}\n\n[ì§ˆë¬¸]\n{question}"
        )
        chain = prompt | LLM_MODEL | StrOutputParser()
        generation = chain.invoke({"context": context, "question": question})
        return {"generation": generation}

    def web_search_node(state: GraphState) -> GraphState:
        try:
            tool = TavilySearchResults(k=3)
            docs = tool.invoke({"query": state["question"]})
            web_content = [d["content"] for d in docs]
            return {"documents": web_content}
        except Exception:
            return {
                "documents": ["ì›¹ ê²€ìƒ‰ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (API Key í™•ì¸ í•„ìš”)."]
            }

    def decide_route(state: GraphState) -> str:
        documents = state.get("documents", [])
        if not documents:
            return "web_search"
        return "generate"

    # --- ê·¸ë˜í”„ ì¡°ë¦½ ---
    workflow = StateGraph(GraphState)
    workflow.add_node("decompose", query_decomposition_node)
    workflow.add_node("retrieve", retrieval_node)
    workflow.add_node("grade", grade_documents_node)
    workflow.add_node("generate", generate_node)
    workflow.add_node("web_search", web_search_node)

    workflow.add_edge(START, "decompose")
    workflow.add_edge("decompose", "retrieve")
    workflow.add_edge("retrieve", "grade")
    workflow.add_conditional_edges(
        "grade",
        decide_route,
        {"web_search": "web_search", "generate": "generate"},
    )
    workflow.add_edge("web_search", "generate")
    workflow.add_edge("generate", END)

    return workflow.compile()


# ==============================================================================
# [Part 3] Streamlit UI êµ¬ì„±
# ==============================================================================

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    # ê°„ë‹¨í•œ dict êµ¬ì¡°ë¡œ ê´€ë¦¬: {"role": "user"|"assistant", "content": str}
    st.session_state["messages"] = []

if "rag_app" not in st.session_state:
    st.session_state["rag_app"] = None


# ì‚¬ì´ë“œë°”: íŒŒì¼ ì—…ë¡œë“œ ë° ì„¤ì •
with st.sidebar:
    st.header("ğŸ“‚ ë¬¸ì„œ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("ì—°êµ¬ ë…¼ë¬¸(PDF)ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])

    if uploaded_file:
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (PyMuPDFLoaderëŠ” ê²½ë¡œê°€ í•„ìš”í•¨)
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

        # íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆì„ ë•Œë§Œ ì¬ë¹Œë“œ
        if (
            "current_file" not in st.session_state
            or st.session_state["current_file"] != uploaded_file.name
        ):
            retriever = build_retriever(tmp_file_path)
            if retriever:
                st.session_state["rag_app"] = create_rag_graph(retriever)
                st.session_state["current_file"] = uploaded_file.name
                st.success("RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
            else:
                st.error("RAG ì‹œìŠ¤í…œ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. PDF ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(tmp_file_path)

    st.divider()
    if st.button("ëŒ€í™” ë‚´ìš© ì´ˆê¸°í™”"):
        st.session_state["messages"] = []
        st.rerun()


# ì±„íŒ… íˆìŠ¤í† ë¦¬ ì¶œë ¥ í•¨ìˆ˜
def print_history():
    for msg in st.session_state["messages"]:
        st.chat_message(msg["role"]).write(msg["content"])


def add_history(role: str, content: str):
    st.session_state["messages"].append({"role": role, "content": content})


# ë©”ì¸ í™”ë©´ ë Œë”ë§
print_history()

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_input := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # 1. ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° ì¶œë ¥
    add_history("user", user_input)
    st.chat_message("user").write(user_input)

    # 2. AI ì‘ë‹µ ìƒì„±
    if st.session_state["rag_app"] is None:
        st.warning("ë¨¼ì € ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    else:
        with st.chat_message("assistant"):
            chat_container = st.empty()

            # LangGraph ì‹¤í–‰ ë° ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜
            inputs = {"question": user_input}
            app = st.session_state["rag_app"]

            # ë‹¨ê³„ë³„ ì§„í–‰ ìƒí™© í‘œì‹œ
            with st.status("AIê°€ ìƒê° ì¤‘...", expanded=True) as status:
                final_answer = ""

                # stream()ì„ ì‚¬ìš©í•˜ì—¬ ë…¸ë“œ ì§„í–‰ ìƒí™©ì„ ë³¼ ìˆ˜ ìˆìŒ
                for output in app.stream(inputs):
                    for key, value in output.items():
                        st.write(f"ğŸš© **{key}** ë‹¨ê³„ ì™„ë£Œ")
                        if key == "generate":
                            final_answer = value["generation"]

                status.update(label="ë‹µë³€ ìƒì„± ì™„ë£Œ", state="complete", expanded=False)

            # ìµœì¢… ë‹µë³€ ì¶œë ¥
            chat_container.markdown(final_answer)
            add_history("assistant", final_answer)
